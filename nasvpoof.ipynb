{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11060404,"sourceType":"datasetVersion","datasetId":6891325}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import roc_curve\nimport soundfile as sf\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport librosa\nimport random\nfrom torch.distributions import Categorical\nimport wandb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed()\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Feature extraction functions\ndef extract_mfcc(audio, sr=16000, n_mfcc=20):\n    \"\"\"Extract MFCC features from audio\"\"\"\n    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n    delta = librosa.feature.delta(mfcc)\n    delta2 = librosa.feature.delta(mfcc, order=2)\n    features = np.concatenate([mfcc, delta, delta2], axis=0)\n    return features\n\ndef extract_spec(audio, sr=16000, n_fft=512, hop_length=256):\n    \"\"\"Extract log mel-spectrogram features from audio\"\"\"\n    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=80)\n    log_mel_spec = librosa.power_to_db(mel_spec)\n    return log_mel_spec\n\ndef extract_cqt(audio, sr=16000, hop_length=256):\n    \"\"\"Extract Constant-Q Transform features from audio\"\"\"\n    cqt = librosa.cqt(y=audio, sr=sr, hop_length=hop_length)\n    return np.abs(cqt)\n\n# ASVSpoof Dataset Class\nclass ASVSpoofDataset(Dataset):\n    def __init__(self, root_dir, protocol_file, feature_type='mfcc', max_len=None, is_train=True, use_subsampling=True ):\n        \"\"\"\n        Args:\n            root_dir (string): Directory with all the audio files.\n            protocol_file (string): Path to the protocol file.\n            feature_type (string): Type of features to extract ('mfcc', 'spec', 'cqt').\n            max_len (int): Maximum length of features sequence.\n            is_train (bool): Whether this is for training or testing.\n        \"\"\"\n        self.root_dir = root_dir\n        self.feature_type = feature_type\n        self.max_len = max_len\n        self.is_train = is_train\n        \n        # Read protocol file\n        self.data = []\n        \n        print(f\"Reading protocol file: {protocol_file}\")\n        try:\n            with open(protocol_file, 'r') as f:\n                lines = f.readlines()\n                \n                # Use tqdm for loading progress\n                for line in tqdm(lines, desc=f\"Loading {'training' if is_train else 'evaluation'} protocol\"):\n                    parts = line.strip().split()\n                    if len(parts) >= 5:\n                        speaker_id = parts[0]\n                        file_id = parts[1]\n                        label_text = parts[4]\n                        label = 0 if label_text == 'bonafide' else 1  # 0 for bonafide, 1 for spoof\n                        self.data.append((file_id, label))\n            \n            # Count number of bonafide and spoof samples\n            bonafide_count = sum(1 for _, label in self.data if label == 0)\n            spoof_count = sum(1 for _, label in self.data if label == 1)\n            \n            print(f\"Dataset loaded: {len(self.data)} samples ({bonafide_count} bonafide, {spoof_count} spoof)\")\n            \n            if is_train and use_subsampling:\n                # Subsample for faster NAS\n                if len(self.data) > 5000:\n                    print(f\"Subsampling training data for faster NAS...\")\n                    np.random.shuffle(self.data)\n                    # Keep balanced class distribution\n                    bonafide_samples = [item for item in self.data if item[1] == 0][:2500]\n                    spoof_samples = [item for item in self.data if item[1] == 1][:2500]\n                    self.data = bonafide_samples + spoof_samples\n                    np.random.shuffle(self.data)\n                    print(f\"Subsampled to {len(self.data)} samples\")\n        \n        except Exception as e:\n            print(f\"Error loading protocol file: {e}\")\n            self.data = []\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        file_id, label = self.data[idx]\n        audio_path = os.path.join(self.root_dir, f\"{file_id}.flac\")\n        \n        try:\n            audio, sr = sf.read(audio_path)\n            \n            # Feature extraction\n            if self.feature_type == 'mfcc':\n                features = extract_mfcc(audio, sr)\n            elif self.feature_type == 'spec':\n                features = extract_spec(audio, sr)\n            elif self.feature_type == 'cqt':\n                features = extract_cqt(audio, sr)\n            else:\n                raise ValueError(f\"Unknown feature type: {self.feature_type}\")\n            \n            # Normalize features\n            features = (features - np.mean(features)) / (np.std(features) + 1e-8)\n            \n            # Handle sequence length\n            seq_len = features.shape[1]\n            if self.max_len is not None:\n                if seq_len > self.max_len:\n                    start = np.random.randint(0, seq_len - self.max_len) if self.is_train else 0\n                    features = features[:, start:start+self.max_len]\n                elif seq_len < self.max_len:\n                    # Pad with zeros\n                    pad_width = ((0, 0), (0, self.max_len - seq_len))\n                    features = np.pad(features, pad_width, mode='constant')\n            \n            return torch.FloatTensor(features), torch.LongTensor([label])[0]\n            \n        except Exception as e:\n            print(f\"Error loading {audio_path}: {e}\")\n            # Return a dummy sample in case of error\n            dummy_features = np.zeros((60, 100 if self.max_len is None else self.max_len))\n            return torch.FloatTensor(dummy_features), torch.LongTensor([label])[0]","metadata":{"execution":{"iopub.status.busy":"2025-04-25T00:08:53.588412Z","iopub.execute_input":"2025-04-25T00:08:53.588632Z","iopub.status.idle":"2025-04-25T00:09:01.304676Z","shell.execute_reply.started":"2025-04-25T00:08:53.588610Z","shell.execute_reply":"2025-04-25T00:09:01.304027Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Neural Architecture Search operations\n\n# Base operation class\nclass Operation(nn.Module):\n    \"\"\"Base class for all operations in the search space\"\"\"\n    def __init__(self, channels, stride=1):\n        super(Operation, self).__init__()\n        self.channels = channels\n        self.stride = stride\n    \n    def forward(self, x):\n        raise NotImplementedError\n\n# Convolutional Block\nclass ConvBlock(Operation):\n    def __init__(self, channels, kernel_size, stride=1):\n        super(ConvBlock, self).__init__(channels, stride)\n        self.conv = nn.Conv1d(channels, channels, kernel_size, stride=stride, padding=kernel_size//2)\n        self.bn = nn.BatchNorm1d(channels)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\n# LSTM Block\nclass LSTM(Operation):\n    def __init__(self, channels, stride=1):\n        super(LSTM, self).__init__(channels, stride)\n        self.lstm = nn.LSTM(channels, channels, batch_first=True)\n        self.input_proj = nn.Conv1d(in_channels=channels, out_channels=channels, kernel_size=1)\n    \n    def forward(self, x):\n        # x shape: [B, C, T]\n        batch_size, channels, seq_len = x.size()\n        x = x.permute(0, 2, 1)  # [B, C, T] -> [B, T, C]\n        \n        # LSTM with batch_first=True\n        x, _ = self.lstm(x)\n        \n        # Return to original dimension ordering\n        x = x.permute(0, 2, 1)  # [B, T, C] -> [B, C, T]\n        \n        return x\n\n# Dilated Convolution\nclass Dilated(Operation):\n    def __init__(self, channels, stride=1):\n        super(Dilated, self).__init__(channels, stride)\n        self.conv = nn.Conv1d(channels, channels, 3, stride=stride, padding=2, dilation=2)\n        self.bn = nn.BatchNorm1d(channels)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\n# Skip Connection\nclass SkipConnect(Operation):\n    def __init__(self, channels, stride=1):\n        super(SkipConnect, self).__init__(channels, stride)\n    \n    def forward(self, x):\n        return x\n\n# Self-Attention Block\nclass Attention(Operation):\n    def __init__(self, channels, stride=1):\n        super(Attention, self).__init__(channels, stride)\n        self.query = nn.Conv1d(channels, channels, 1)\n        self.key = nn.Conv1d(channels, channels, 1)\n        self.value = nn.Conv1d(channels, channels, 1)\n        self.scale = torch.sqrt(torch.FloatTensor([channels])).to(device)\n    \n    def forward(self, x):\n        # x shape: [B, C, T]\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        \n        # Reshape for attention\n        batch_size, C, T = q.size()\n        q = q.permute(0, 2, 1)  # [B, T, C]\n        k = k.permute(0, 2, 1)  # [B, T, C]\n        v = v.permute(0, 2, 1)  # [B, T, C]\n        \n        # Self-attention\n        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n        attention = F.softmax(scores, dim=-1)\n        context = torch.matmul(attention, v)\n        \n        # Reshape back\n        return context.permute(0, 2, 1)  # [B, C, T]\n\n# Separable Convolution\nclass SeparableConv(Operation):\n    def __init__(self, channels, stride=1):\n        super(SeparableConv, self).__init__(channels, stride)\n        self.depthwise = nn.Conv1d(channels, channels, 3, stride=stride, padding=1, groups=channels)\n        self.pointwise = nn.Conv1d(channels, channels, 1)\n        self.bn = nn.BatchNorm1d(channels)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        return self.relu(self.bn(self.pointwise(self.depthwise(x))))\n\n# Squeeze-and-Excitation Block\nclass SqueezeExcitation(Operation):\n    def __init__(self, channels, stride=1, reduction=16):\n        super(SqueezeExcitation, self).__init__(channels, stride)\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Conv1d(channels, max(channels // reduction, 1), kernel_size=1)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Conv1d(max(channels // reduction, 1), channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        # x shape: [B, C, T]\n        scale = self.avg_pool(x)\n        scale = self.fc1(scale)\n        scale = self.relu(scale)\n        scale = self.fc2(scale)\n        scale = self.sigmoid(scale)\n        return x * scale\n\n# Frequency-Aware Convolution (Audio-specific)\nclass FrequencyAwareConv(Operation):\n    def __init__(self, channels, stride=1, bands=4):\n        super(FrequencyAwareConv, self).__init__(channels, stride)\n        # Ensure band_size is at least 1\n        self.band_size = max(channels // bands, 1)\n        self.bands = min(bands, channels)\n        \n        # Create different kernel sizes for frequency bands\n        self.convs = nn.ModuleList([\n            nn.Conv1d(self.band_size, self.band_size, 3 + i*2, padding=(3+i*2)//2, stride=stride)\n            for i in range(self.bands)\n        ])\n        \n        self.bn = nn.BatchNorm1d(channels)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        # Split along channel dimension into bands\n        split_sizes = [self.band_size] * self.bands\n        remaining = self.channels - (self.band_size * self.bands)\n        if remaining > 0:\n            split_sizes[-1] += remaining\n            \n        x_bands = torch.split(x, split_sizes, dim=1)\n        \n        # Process each band separately\n        out_bands = []\n        for i, band in enumerate(x_bands):\n            if i < self.bands:\n                out_bands.append(self.convs[i](band))\n        \n        # Concatenate results\n        out = torch.cat(out_bands, dim=1)\n        \n        return self.relu(self.bn(out))\n\n# Gated Convolution\nclass GatedConv(Operation):\n    def __init__(self, channels, stride=1):\n        super(GatedConv, self).__init__(channels, stride)\n        self.conv_features = nn.Conv1d(channels, channels, 3, stride=stride, padding=1)\n        self.conv_gate = nn.Conv1d(channels, channels, 3, stride=stride, padding=1)\n        self.bn = nn.BatchNorm1d(channels)\n    \n    def forward(self, x):\n        features = self.conv_features(x)\n        gate = torch.sigmoid(self.conv_gate(x))\n        return self.bn(features * gate)\n\n# Mixed Operation (Weighted sum of operations)\nclass MixedOp(nn.Module):\n    def __init__(self, channels, stride=1):\n        super(MixedOp, self).__init__()\n        self.ops = nn.ModuleList([\n            # Original operations\n            ConvBlock(channels, 3, stride),\n            ConvBlock(channels, 5, stride),\n            LSTM(channels, stride),\n            Dilated(channels, stride),\n            SkipConnect(channels, stride),\n            Attention(channels, stride),\n            # New operations\n            SeparableConv(channels, stride),\n            SqueezeExcitation(channels, stride),\n            FrequencyAwareConv(channels, stride),\n            GatedConv(channels, stride)\n        ])\n    \n    def forward(self, x, weights):\n        \"\"\"Forward pass with operation weights\"\"\"\n        return sum(w * op(x) for w, op in zip(weights, self.ops))\n\n# Cell structure\nclass Cell(nn.Module):\n    def __init__(self, channels, num_nodes=4):\n        super(Cell, self).__init__()\n        self.channels = channels\n        self.num_nodes = num_nodes\n        \n        # For each node, create edges from all previous nodes\n        self.edges = nn.ModuleList()\n        for i in range(num_nodes):\n            for j in range(i+1):  # connections from input and previous nodes\n                self.edges.append(MixedOp(channels))\n        \n        # Output projection\n        self.project = nn.Conv1d(channels * num_nodes, channels, 1)\n    \n    def forward(self, x, weights):\n        \"\"\"\n        Forward pass through the cell\n        Args:\n            x: Input tensor [B, C, T]\n            weights: List of weight tensors for each edge\n        \"\"\"\n        states = [x]\n        offset = 0\n        \n        # Process each node\n        for i in range(self.num_nodes):\n            # Gather inputs from previous nodes\n            node_inputs = []\n            for j in range(i+1):\n                edge_output = self.edges[offset + j](states[j], weights[offset + j])\n                node_inputs.append(edge_output)\n            \n            node_input = sum(node_inputs)\n            offset += i+1\n            states.append(node_input)\n        \n        # Concatenate all intermediate nodes\n        cat_states = torch.cat(states[1:], dim=1)\n        \n        return self.project(cat_states)","metadata":{"execution":{"iopub.status.busy":"2025-04-25T00:09:39.376169Z","iopub.execute_input":"2025-04-25T00:09:39.376479Z","iopub.status.idle":"2025-04-25T00:09:39.401244Z","shell.execute_reply.started":"2025-04-25T00:09:39.376456Z","shell.execute_reply":"2025-04-25T00:09:39.400549Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Complete model with hybrid PPO-DARTS support\nclass DeepfakeDetectionModel(nn.Module):\n    def __init__(self, input_channels, num_cells=3, num_nodes=4, num_ops=10):\n        super(DeepfakeDetectionModel, self).__init__()\n        self.input_channels = input_channels\n        self.num_cells = num_cells\n        self.num_nodes = num_nodes\n        self.num_ops = num_ops\n        \n        # Initial projection\n        self.stem = nn.Sequential(\n            nn.Conv1d(input_channels, 64, 3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU()\n        )\n        \n        # Cells\n        self.cells = nn.ModuleList()\n        for i in range(num_cells):\n            self.cells.append(Cell(64, num_nodes))\n        \n        # Classification head\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(64, 2)  # Binary classification\n        \n        # Initialize architectural parameters (alphas) for DARTS\n        self._initialize_alphas()\n        \n        # Calculate total number of weights needed for PPO\n        edges_per_cell = sum(range(1, num_nodes+1))\n        self.total_weights = num_cells * edges_per_cell * num_ops\n    \n    def _initialize_alphas(self):\n        \"\"\"Initialize architectural parameters for DARTS\"\"\"\n        edges_per_cell = sum(range(1, self.num_nodes+1))\n        total_edges = self.num_cells * edges_per_cell\n        # Create parameter tensor for alphas\n        self._alphas = nn.Parameter(torch.zeros(total_edges, self.num_ops))\n        # Initialize with small random values\n        nn.init.normal_(self._alphas, mean=0, std=0.001)\n    \n    def alphas(self):\n        \"\"\"Return architectural parameters for optimizer\"\"\"\n        return [self._alphas]  # Wrapped in list for optimizer compatibility\n    \n    def weights(self):\n        \"\"\"Return model weights excluding alphas\"\"\"\n        return [p for n, p in self.named_parameters() if '_alphas' not in n]\n    \n    def forward(self, x, architecture_weights=None, discrete=False):\n        \"\"\"\n        Forward pass with multiple modes:\n        - PPO mode: Using external architecture_weights\n        - DARTS mode: Using internal alphas with continuous relaxation\n        - Evaluation mode: Using internal alphas with discrete operations\n        \"\"\"\n        # Input shape handling\n        if x.shape[1] == self.input_channels:\n            # Input is already [B, C, T]\n            pass\n        else:\n            # Input is [B, T, C], convert to [B, C, T]\n            x = x.permute(0, 2, 1)\n        \n        # Process input\n        x = self.stem(x)\n        \n        # Determine which weights to use\n        edges_per_cell = sum(range(1, self.num_nodes+1))\n        \n        if architecture_weights is not None:\n            # PPO mode: use external weights\n            edge_weights = []\n            for i in range(len(architecture_weights) // self.num_ops):\n                start_idx = i * self.num_ops\n                end_idx = start_idx + self.num_ops\n                # Apply softmax to get probability distribution\n                edge_weights.append(F.softmax(architecture_weights[start_idx:end_idx], dim=0))\n        else:\n            # DARTS mode: use internal alphas\n            if discrete:\n                # Convert to discrete (one-hot) for evaluation\n                max_indices = torch.argmax(self._alphas, dim=1)\n                edge_weights = []\n                for j, idx in enumerate(max_indices):\n                    weights = torch.zeros_like(self._alphas[j])\n                    weights[idx] = 1.0\n                    edge_weights.append(weights)\n            else:\n                # Use softmax for continuous relaxation\n                edge_weights = [F.softmax(self._alphas[i], dim=0) for i in range(self._alphas.size(0))]\n        \n        # Process cells\n        offset = 0\n        for i, cell in enumerate(self.cells):\n            cell_weights = edge_weights[offset:offset + edges_per_cell]\n            offset += edges_per_cell\n            x = cell(x, cell_weights)\n        \n        # Classification\n        x = self.pool(x).squeeze(-1)\n        x = self.classifier(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2025-04-25T00:09:42.380437Z","iopub.execute_input":"2025-04-25T00:09:42.380974Z","iopub.status.idle":"2025-04-25T00:09:42.391649Z","shell.execute_reply.started":"2025-04-25T00:09:42.380953Z","shell.execute_reply":"2025-04-25T00:09:42.390994Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport networkx as nx\nfrom matplotlib.patches import FancyArrowPatch\n\ndef visualize_architecture(architecture, num_cells=3, num_nodes=4, num_ops=10, save_path='architecture_visualization.png', \n                           save_to_wandb=False, title=\"Hybrid PPO-DARTS Architecture\"):\n    \"\"\"\n    Visualize the architecture discovered by the hybrid PPO-DARTS approach.\n    \n    Args:\n        architecture: Tensor containing the selected operations (for PPO) or operation weights (for DARTS)\n        num_cells: Number of cells in the architecture\n        num_nodes: Number of intermediate nodes in each cell\n        num_ops: Number of possible operations for each edge\n        save_path: Path to save the visualization\n        save_to_wandb: Whether to save the visualization to wandb\n        title: Title of the visualization\n    \n    Returns:\n        Path to the saved visualization\n    \"\"\"\n    # Define operation names for visualization\n    operation_names = [\n        'Conv 3x3', 'Conv 5x5', 'LSTM', 'Dilated Conv', 'Skip Connect',\n        'Self Attention', 'Separable Conv', 'Squeeze-Excitation', 'Frequency-Aware', 'Gated Conv'\n    ]\n    \n    # Define colors for different operations\n    colors = list(mcolors.TABLEAU_COLORS.values())\n    \n    # Calculate edges per cell\n    edges_per_cell = sum(range(1, num_nodes+1))\n    total_edges = num_cells * edges_per_cell\n    \n    # Check if architecture is from PPO (1D tensor of indices) or DARTS (2D tensor of weights)\n    is_ppo = architecture.dim() == 1\n    \n    # Create a figure with multiple subplots (one per cell)\n    fig, axes = plt.subplots(1, num_cells, figsize=(6*num_cells, 6), constrained_layout=True)\n    if num_cells == 1:\n        axes = [axes]\n    \n    plt.suptitle(title, fontsize=20, y=1.05)\n    \n    # For each cell\n    for cell_idx in range(num_cells):\n        ax = axes[cell_idx]\n        G = nx.DiGraph()\n        \n        # Label for cell type\n        cell_type = \"Normal Cell\" if cell_idx % 2 == 0 else \"Expand Cell\"\n        ax.set_title(f\"Cell {cell_idx+1}: {cell_type}\", fontsize=16)\n        \n        # Add nodes to the graph\n        for i in range(num_nodes + 2):  # +2 for input and output nodes\n            if i == 0 or i == 1:\n                G.add_node(i, label=f\"Input {i+1}\")\n            elif i == num_nodes + 1:\n                G.add_node(i, label=\"Output\")\n            else:\n                G.add_node(i, label=f\"Node {i}\")\n        \n        # Add edges to the graph based on the architecture\n        edge_offset = cell_idx * edges_per_cell\n        edge_count = 0\n        \n        for i in range(2, num_nodes + 2):  # For each intermediate node\n            for j in range(i):  # For all previous nodes\n                edge_idx = edge_offset + edge_count\n                \n                if is_ppo:\n                    # For PPO, the architecture contains operation indices\n                    if edge_idx < len(architecture):\n                        op_idx = int(architecture[edge_idx].item())\n                        op_name = operation_names[op_idx]\n                        G.add_edge(j, i, label=op_name, color=colors[op_idx])\n                else:\n                    # For DARTS, the architecture contains operation weights\n                    if edge_idx < architecture.size(0):\n                        op_idx = torch.argmax(architecture[edge_idx]).item()\n                        op_name = operation_names[op_idx]\n                        G.add_edge(j, i, label=op_name, color=colors[op_idx])\n                \n                edge_count += 1\n        \n        # Position nodes in a hierarchical layout\n        pos = {}\n        pos[0] = np.array([-1, 0.5])\n        pos[1] = np.array([-1, -0.5])\n        \n        # Position intermediate nodes in a line\n        for i in range(2, num_nodes + 2):\n            level = (i - 1) / (num_nodes + 1)\n            pos[i] = np.array([level*2 - 1, 0])\n        \n        # Adjust output node position\n        pos[num_nodes + 1] = np.array([1, 0])\n        \n        # Draw nodes\n        for n in G.nodes:\n            nx.draw_networkx_nodes(G, pos, nodelist=[n], node_size=1200, \n                                  node_color='lightblue', alpha=0.8, ax=ax)\n        \n        # Draw node labels\n        nx.draw_networkx_labels(G, pos, labels=nx.get_node_attributes(G, 'label'), \n                               font_size=10, font_family='sans-serif', ax=ax)\n        \n        # Draw edges with custom arrows\n        for u, v, data in G.edges(data=True):\n            color = data.get('color', 'gray')\n            label = data.get('label', '')\n            \n            # Create a curved arrow\n            arrow = FancyArrowPatch(pos[u], pos[v], connectionstyle=\"arc3,rad=0.2\",\n                                   arrowstyle=\"-|>\", color=color, lw=1.5, alpha=0.8)\n            ax.add_patch(arrow)\n            \n            # Add edge label (operation name)\n            # Calculate label position (midpoint of the curved edge with slight offset)\n            x = (pos[u][0] + pos[v][0]) / 2\n            y = (pos[u][1] + pos[v][1]) / 2\n            offset = 0.1 if pos[u][1] < pos[v][1] else -0.1\n            ax.text(x, y + offset, label, fontsize=8, ha='center', va='center', \n                   bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))\n        \n        # Remove axis ticks and frame\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n    \n    # Save figure\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    # Log to wandb if requested\n    if save_to_wandb:\n        try:\n            import wandb\n            if wandb.run is not None:\n                wandb.log({\"architecture_visualization\": wandb.Image(save_path)})\n        except ImportError:\n            print(\"Warning: wandb not installed, skipping wandb logging\")\n    \n    plt.close()\n    return save_path\n\n\ndef visualize_darts_architecture(architecture_weights, num_cells=3, num_nodes=4, num_ops=10, \n                                save_path='darts_architecture.png', save_to_wandb=False):\n    \"\"\"\n    Visualize the DARTS architecture represented by architecture weights.\n    \n    Args:\n        architecture_weights: Tensor of shape [num_edges, num_ops] containing operation weights\n        num_cells: Number of cells in the architecture\n        num_nodes: Number of nodes in each cell\n        num_ops: Number of operations\n    \"\"\"\n    return visualize_architecture(\n        architecture_weights, \n        num_cells, \n        num_nodes, \n        num_ops, \n        save_path, \n        save_to_wandb,\n        title=\"DARTS Architecture (Discrete)\"\n    )\n\n\ndef visualize_ppo_architecture(architecture_indices, num_cells=3, num_nodes=4, num_ops=10, \n                              save_path='ppo_architecture.png', save_to_wandb=False):\n    \"\"\"\n    Visualize the PPO architecture represented by operation indices.\n    \n    Args:\n        architecture_indices: Tensor containing operation indices for each edge\n        num_cells: Number of cells in the architecture\n        num_nodes: Number of nodes in each cell\n        num_ops: Number of operations\n    \"\"\"\n    return visualize_architecture(\n        architecture_indices, \n        num_cells, \n        num_nodes, \n        num_ops, \n        save_path, \n        save_to_wandb,\n        title=\"PPO-Generated Architecture\"\n    )\n\n\ndef compare_architectures(ppo_architecture, darts_architecture, num_cells=3, num_nodes=4, num_ops=10,\n                         save_path='architecture_comparison.png', save_to_wandb=False):\n    \"\"\"\n    Create a visualization comparing PPO and DARTS architectures side by side.\n    \n    Args:\n        ppo_architecture: Architecture tensor from PPO\n        darts_architecture: Architecture tensor from DARTS\n        num_cells: Number of cells\n        num_nodes: Number of nodes per cell\n        num_ops: Number of operations\n    \"\"\"\n    # Define operation names\n    operation_names = [\n        'Conv 3x3', 'Conv 5x5', 'LSTM', 'Dilated Conv', 'Skip Connect',\n        'Self Attention', 'Separable Conv', 'Squeeze-Excitation', 'Frequency-Aware', 'Gated Conv'\n    ]\n    \n    # Create a figure with a grid of subplots\n    fig, axes = plt.subplots(2, num_cells, figsize=(6*num_cells, 12), constrained_layout=True)\n    plt.suptitle(\"PPO vs DARTS Architecture Comparison\", fontsize=24, y=1.05)\n    \n    # Top row for PPO\n    for cell_idx in range(num_cells):\n        axes[0, cell_idx].set_title(f\"PPO Cell {cell_idx+1}\", fontsize=16)\n    \n    # Bottom row for DARTS\n    for cell_idx in range(num_cells):\n        axes[1, cell_idx].set_title(f\"DARTS Cell {cell_idx+1}\", fontsize=16)\n    \n    # Save paths for individual visualizations\n    ppo_save_path = 'ppo_temp.png'\n    darts_save_path = 'darts_temp.png'\n    \n    # Generate the individual visualizations\n    visualize_ppo_architecture(ppo_architecture, num_cells, num_nodes, num_ops, ppo_save_path)\n    visualize_darts_architecture(darts_architecture, num_cells, num_nodes, num_ops, darts_save_path)\n    \n    # Create a combined visualization\n    from PIL import Image\n    ppo_img = Image.open(ppo_save_path)\n    darts_img = Image.open(darts_save_path)\n    \n    # Create a new combined image\n    combined_width = max(ppo_img.width, darts_img.width)\n    combined_height = ppo_img.height + darts_img.height + 50  # Extra space for title\n    combined_img = Image.new('RGB', (combined_width, combined_height), color='white')\n    \n    # Add title\n    from PIL import ImageDraw, ImageFont\n    draw = ImageDraw.Draw(combined_img)\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 36)\n    except:\n        font = ImageFont.load_default()\n    \n    draw.text((combined_width//2 - 200, 10), \"Architecture Comparison\", fill=\"black\", font=font)\n    \n    # Paste the individual images\n    combined_img.paste(ppo_img, (0, 50))\n    combined_img.paste(darts_img, (0, 50 + ppo_img.height))\n    \n    # Save the combined image\n    combined_img.save(save_path)\n    \n    # Clean up temporary files\n    os.remove(ppo_save_path)\n    os.remove(darts_save_path)\n    \n    # Log to wandb if requested\n    if save_to_wandb:\n        try:\n            import wandb\n            if wandb.run is not None:\n                wandb.log({\"architecture_comparison\": wandb.Image(save_path)})\n        except ImportError:\n            print(\"Warning: wandb not installed, skipping wandb logging\")\n    \n    return save_path\n\n\ndef analyze_architecture_statistics(best_architecture, is_ppo=True, num_cells=3, num_nodes=4, num_ops=10):\n    \"\"\"\n    Analyze the statistics of the discovered architecture.\n    \n    Args:\n        best_architecture: The architecture tensor (PPO indices or DARTS weights)\n        is_ppo: Whether the architecture is from PPO (True) or DARTS (False)\n        num_cells: Number of cells\n        num_nodes: Number of nodes per cell\n        num_ops: Number of operations\n    \n    Returns:\n        Dictionary of statistics about the architecture\n    \"\"\"\n    operation_names = [\n        'Conv 3x3', 'Conv 5x5', 'LSTM', 'Dilated Conv', 'Skip Connect',\n        'Self Attention', 'Separable Conv', 'Squeeze-Excitation', 'Frequency-Aware', 'Gated Conv'\n    ]\n    \n    # Calculate edges per cell\n    edges_per_cell = sum(range(1, num_nodes+1))\n    total_edges = num_cells * edges_per_cell\n    \n    # Initialize operation counts\n    op_counts = {op: 0 for op in operation_names}\n    \n    # Count operations by cell\n    op_counts_by_cell = []\n    for cell_idx in range(num_cells):\n        cell_op_counts = {op: 0 for op in operation_names}\n        edge_offset = cell_idx * edges_per_cell\n        \n        for edge_idx in range(edges_per_cell):\n            global_edge_idx = edge_offset + edge_idx\n            \n            if is_ppo:\n                # For PPO architecture (indices)\n                if global_edge_idx < len(best_architecture):\n                    op_idx = int(best_architecture[global_edge_idx].item())\n                    op_name = operation_names[op_idx]\n                    op_counts[op_name] += 1\n                    cell_op_counts[op_name] += 1\n            else:\n                # For DARTS architecture (weights)\n                if global_edge_idx < best_architecture.size(0):\n                    op_idx = torch.argmax(best_architecture[global_edge_idx]).item()\n                    op_name = operation_names[op_idx]\n                    op_counts[op_name] += 1\n                    cell_op_counts[op_name] += 1\n        \n        op_counts_by_cell.append(cell_op_counts)\n    \n    # Calculate percentages\n    total_ops = sum(op_counts.values())\n    op_percentages = {op: count/total_ops*100 for op, count in op_counts.items()}\n    \n    # Find most and least common operations\n    most_common_op = max(op_counts.items(), key=lambda x: x[1])[0]\n    least_common_op = min(op_counts.items(), key=lambda x: x[1])[0]\n    \n    # Analyze patterns\n    patterns = {}\n    patterns[\"most_common_op\"] = most_common_op\n    patterns[\"most_common_percentage\"] = op_percentages[most_common_op]\n    patterns[\"least_common_op\"] = least_common_op\n    patterns[\"least_common_percentage\"] = op_percentages[least_common_op]\n    \n    # Check for cell-specific patterns\n    patterns[\"cell_specific_patterns\"] = []\n    for i, cell_counts in enumerate(op_counts_by_cell):\n        cell_total = sum(cell_counts.values())\n        if cell_total > 0:\n            cell_most_common = max(cell_counts.items(), key=lambda x: x[1])[0]\n            cell_percent = cell_counts[cell_most_common] / cell_total * 100\n            if cell_percent > 40:  # If an operation dominates a cell\n                patterns[\"cell_specific_patterns\"].append(\n                    f\"Cell {i+1} ({['Normal', 'Expand'][i%2]}) uses {cell_most_common} for {cell_percent:.1f}% of connections\"\n                )\n    \n    # Return combined statistics\n    statistics = {\n        \"operation_counts\": op_counts,\n        \"operation_percentages\": op_percentages,\n        \"patterns\": patterns,\n        \"by_cell\": op_counts_by_cell\n    }\n    \n    return statistics\n\n\ndef plot_architecture_statistics(statistics, save_path='architecture_stats.png', save_to_wandb=False):\n    \"\"\"\n    Create visualizations of architecture statistics.\n    \n    Args:\n        statistics: Output from analyze_architecture_statistics\n        save_path: Path to save the visualization\n        save_to_wandb: Whether to log to wandb\n    \"\"\"\n    # Create a figure with subplots\n    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n    \n    # Plot operation counts\n    op_counts = statistics[\"operation_counts\"]\n    sorted_ops = sorted(op_counts.items(), key=lambda x: x[1], reverse=True)\n    ops, counts = zip(*sorted_ops)\n    \n    axes[0].bar(ops, counts, color='skyblue')\n    axes[0].set_title('Operation Counts', fontsize=16)\n    axes[0].set_ylabel('Count')\n    axes[0].tick_params(axis='x', rotation=45)\n    \n    # Annotate bars with counts\n    for i, v in enumerate(counts):\n        axes[0].text(i, v + 0.1, str(v), ha='center')\n    \n    # Plot operation percentages by cell\n    cell_data = statistics[\"by_cell\"]\n    cell_names = [f\"Cell {i+1}\\n({'Normal' if i%2==0 else 'Expand'})\" for i in range(len(cell_data))]\n    \n    # Get all operations used\n    all_ops = set()\n    for cell in cell_data:\n        for op, count in cell.items():\n            if count > 0:\n                all_ops.add(op)\n    \n    # Create a grouped bar chart\n    x = np.arange(len(cell_names))\n    width = 0.8 / len(all_ops)\n    \n    # Sort operations by overall frequency\n    sorted_all_ops = sorted(all_ops, key=lambda op: -statistics[\"operation_counts\"].get(op, 0))\n    \n    for i, op in enumerate(sorted_all_ops):\n        values = [cell.get(op, 0) for cell in cell_data]\n        offset = i * width - (len(all_ops) - 1) * width / 2\n        axes[1].bar(x + offset, values, width, label=op)\n    \n    axes[1].set_title('Operations by Cell Type', fontsize=16)\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels(cell_names)\n    axes[1].set_ylabel('Count')\n    axes[1].legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    # Log to wandb if requested\n    if save_to_wandb:\n        try:\n            import wandb\n            if wandb.run is not None:\n                wandb.log({\"architecture_statistics\": wandb.Image(save_path)})\n        except ImportError:\n            print(\"Warning: wandb not installed, skipping wandb logging\")\n    \n    plt.close()\n    return save_path\n\n\n# # Example usage:\n# if __name__ == \"__main__\":\n#     # Example of using these functions with your model:\n    \n#     # 1. Load your best architecture from checkpoint\n#     checkpoint_path = 'best_hybrid_model.pth'\n    \n#     if os.path.exists(checkpoint_path):\n#         checkpoint = torch.load(checkpoint_path)\n        \n#         # Check if it's a PPO or DARTS architecture\n#         if 'ppo_architecture' in checkpoint:\n#             # PPO architecture\n#             architecture = checkpoint['ppo_architecture']\n#             is_ppo = True\n#             best_mode = 'PPO'\n#         elif 'darts_alphas' in checkpoint:\n#             # DARTS architecture\n#             architecture = checkpoint['darts_alphas']\n#             is_ppo = False\n#             best_mode = 'DARTS'\n#         else:\n#             # Example random architecture for demonstration\n#             print(\"No architecture found in checkpoint, generating random example\")\n#             architecture = torch.randint(0, 10, (30,))\n#             is_ppo = True\n#             best_mode = 'Random'\n            \n#         # Visualize the architecture\n#         if is_ppo:\n#             save_path = visualize_ppo_architecture(architecture, save_to_wandb=True)\n#         else:\n#             save_path = visualize_darts_architecture(architecture, save_to_wandb=True)\n            \n#         print(f\"Architecture visualization saved to {save_path}\")\n        \n#         # Analyze architecture statistics\n#         stats = analyze_architecture_statistics(architecture, is_ppo=is_ppo)\n#         stats_path = plot_architecture_statistics(stats, save_to_wandb=True)\n        \n#         print(f\"Architecture statistics saved to {stats_path}\")\n        \n#         # Print key findings\n#         patterns = stats[\"patterns\"]\n#         print(f\"\\nArchitecture Analysis ({best_mode} architecture):\")\n#         print(f\"Most common operation: {patterns['most_common_op']} ({patterns['most_common_percentage']:.1f}%)\")\n#         print(f\"Least common operation: {patterns['least_common_op']} ({patterns['least_common_percentage']:.1f}%)\")\n        \n#         if patterns[\"cell_specific_patterns\"]:\n#             print(\"\\nCell-specific patterns:\")\n#             for pattern in patterns[\"cell_specific_patterns\"]:\n#                 print(f\"- {pattern}\")\n#     else:\n#         print(f\"Checkpoint file {checkpoint_path} not found.\")\n#         print(\"To visualize your architecture, run this script after training or modify the path.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T00:09:42.968218Z","iopub.execute_input":"2025-04-25T00:09:42.968906Z","iopub.status.idle":"2025-04-25T00:09:43.151540Z","shell.execute_reply.started":"2025-04-25T00:09:42.968882Z","shell.execute_reply":"2025-04-25T00:09:43.150798Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# PPO Controller for architecture search\nclass PPOController(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=64):\n        super(PPOController, self).__init__()\n        \n        # Print dimensions for debugging\n        print(f\"Initializing PPO controller with state_dim={state_dim}, action_dim={action_dim}\")\n        \n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n        \n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n    def forward(self, state):\n        # Returns action probabilities and estimated value\n        action_probs = F.softmax(self.actor(state), dim=-1)\n        value = self.critic(state)\n        return action_probs, value\n    \n    def act(self, state):\n        action_probs, _ = self.forward(state)\n        dist = Categorical(action_probs)\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n        return action.detach(), log_prob.detach()\n\n# PPO Training function\ndef train_ppo(controller, optimizer, memories, clip_ratio=0.2, epochs=10, entropy_coef=0.01):\n    \"\"\"Train the PPO controller on collected experiences\"\"\"\n    # Unpack memories\n    states = torch.cat([m['state'] for m in memories])\n    actions = torch.cat([m['action'] for m in memories])\n    old_log_probs = torch.cat([m['log_prob'] for m in memories])\n    rewards = torch.cat([m['reward'] for m in memories])\n    \n    # Normalize rewards for stable training\n    if rewards.std() > 0:\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n    \n    # Store metrics for logging\n    metrics = {\n        'actor_loss': 0,\n        'critic_loss': 0,\n        'entropy_loss': 0\n    }\n    \n    # Train for multiple epochs\n    for _ in range(epochs):\n        # Evaluate current policy\n        log_probs = []\n        values = []\n        entropy = []\n        \n        for i in range(len(states)):\n            state_i = states[i:i+1]\n            action_i = actions[i]\n            \n            # Get action probabilities and value\n            action_probs, value = controller(state_i)\n            \n            # Create categorical distribution\n            dist = Categorical(action_probs)\n            \n            # Get log probability and entropy\n            log_prob = dist.log_prob(action_i)\n            entropy_i = dist.entropy()\n            \n            log_probs.append(log_prob)\n            values.append(value.squeeze())\n            entropy.append(entropy_i)\n        \n        # Stack results\n        log_probs = torch.stack(log_probs)\n        values = torch.stack(values)\n        entropy = torch.stack(entropy)\n        \n        # Compute ratio and surrogate loss\n        ratio = torch.exp(log_probs - old_log_probs)\n        surr1 = ratio * rewards\n        surr2 = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * rewards\n        \n        # PPO losses\n        actor_loss = -torch.min(surr1, surr2).mean()\n        critic_loss = F.mse_loss(values, rewards)\n        entropy_loss = -entropy.mean()\n        \n        # Total loss\n        loss = actor_loss + 0.5 * critic_loss - entropy_coef * entropy_loss\n        \n        # Update controller\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Update metrics\n        metrics['actor_loss'] += actor_loss.item() / epochs\n        metrics['critic_loss'] += critic_loss.item() / epochs\n        metrics['entropy_loss'] += entropy_loss.item() / epochs\n    \n    return metrics['actor_loss'], metrics['critic_loss'], metrics['entropy_loss']","metadata":{"execution":{"iopub.status.busy":"2025-04-25T00:09:53.075786Z","iopub.execute_input":"2025-04-25T00:09:53.076462Z","iopub.status.idle":"2025-04-25T00:09:53.088986Z","shell.execute_reply.started":"2025-04-25T00:09:53.076437Z","shell.execute_reply":"2025-04-25T00:09:53.088226Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Evaluation helper function\ndef evaluate_architecture(model, val_loader, device, architecture_weights=None, discrete=False):\n    \"\"\"Evaluate the performance of an architecture\"\"\"\n    model.eval()\n    all_targets = []\n    all_scores = []\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            try:\n                inputs, targets = inputs.to(device), targets.to(device)\n                if torch.isnan(inputs).any():\n                    inputs = torch.nan_to_num(inputs, nan=0.0)\n                \n                # Use the appropriate mode\n                if architecture_weights is not None:\n                    # PPO mode with external weights\n                    outputs = model(inputs, architecture_weights)\n                else:\n                    # DARTS mode with internal alphas\n                    outputs = model(inputs, discrete=discrete)\n                \n                scores = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n                all_targets.extend(targets.cpu().numpy())\n                all_scores.extend(scores)\n                \n            except Exception as e:\n                print(f\"Error in evaluation: {e}\")\n                continue\n    \n    # Calculate EER with robust error handling\n    try:\n        if len(all_targets) > 0 and len(all_scores) > 0:\n            unique_targets = np.unique(all_targets)\n            if len(unique_targets) >= 2:\n                fpr, tpr, thresholds = roc_curve(all_targets, all_scores, pos_label=1)\n                fnr = 1 - tpr\n                idx = np.nanargmin(np.absolute(fnr - fpr))\n                eer = (fpr[idx] + fnr[idx]) / 2\n            else:\n                eer = 0.5\n        else:\n            eer = 0.5\n    except Exception as e:\n        print(f\"Error calculating EER: {e}\")\n        eer = 0.5\n    \n    return eer\n\n# Hybrid PPO-DARTS search function\ndef search_architecture_hybrid(train_loader, val_loader, device, input_channels=60, num_cells=3, \n                              num_nodes=4, num_ops=10, epochs=30, ppo_updates=5, \n                              project_name=\"deepfake-nas-hybrid\"):\n    \"\"\"Hybrid approach combining PPO for exploration with DARTS for optimization\"\"\"\n    # Initialize wandb\n    wandb.init(project=project_name, name=f\"Hybrid_PPO_DARTS_cells{num_cells}_nodes{num_nodes}\")\n    \n    # Log hyperparameters\n    config = {\n        \"input_channels\": input_channels,\n        \"num_cells\": num_cells,\n        \"num_nodes\": num_nodes,\n        \"num_ops\": num_ops,\n        \"epochs\": epochs,\n        \"w_lr\": 0.001,        # Weight learning rate\n        \"alpha_lr\": 0.0003,   # Architecture parameter learning rate\n        \"ppo_lr\": 0.0005,     # PPO controller learning rate\n        \"ppo_updates\": ppo_updates,\n        \"exploration_ratio\": 0.3,  # Ratio of epochs to use PPO exploration\n        \"visualization_enabled\": True  # Enable visualization\n    }\n    wandb.config.update(config)\n    \n    # Initialize model with expanded operation set\n    model = DeepfakeDetectionModel(input_channels, num_cells, num_nodes, num_ops).to(device)\n    \n    # Calculate edges for PPO controller\n    edges_per_cell = sum(range(1, num_nodes+1))\n    total_edges = num_cells * edges_per_cell\n    \n    # Initialize PPO controller for exploration\n    state_dim = 1  # Single value for validation performance\n    action_dim = num_ops  # Number of operations per edge\n    controller = PPOController(state_dim, action_dim).to(device)\n    controller_optimizer = optim.Adam(controller.parameters(), lr=config[\"ppo_lr\"])\n    \n    # Setup optimizers for DARTS\n    w_optimizer = optim.Adam(model.weights(), lr=config[\"w_lr\"], weight_decay=3e-4)\n    w_scheduler = optim.lr_scheduler.CosineAnnealingLR(w_optimizer, epochs)\n    alpha_optimizer = optim.Adam(model.alphas(), lr=config[\"alpha_lr\"], betas=(0.5, 0.999), weight_decay=1e-3)\n    \n    # Metrics tracking\n    best_val_eer = 1.0\n    best_architecture = None\n    best_mode = None\n    \n    with tqdm(total=epochs, desc=\"Hybrid Search Progress\", position=0, leave=True) as epoch_pbar:\n        for epoch in range(epochs):\n            # Determine exploration mode for this epoch\n            # More exploration in early stages, more exploitation later\n            use_ppo = (random.random() < config[\"exploration_ratio\"] * (1 - epoch/epochs))\n            \n            # PPO exploration phase\n            if use_ppo:\n                model.train()\n                train_loss = 0.0\n                batch_count = 0\n                \n                # For PPO\n                memories = []\n                current_architecture = []\n                \n\n\n\n                # Sample architecture using PPO\n                for i in range(total_edges):\n                    # Use current validation EER as state\n                    state = torch.FloatTensor([min(best_val_eer, 0.5) * 2]).to(device)\n                    \n                    # Sample architecture weights for this edge\n                    for j in range(num_ops):\n                        action, log_prob = controller.act(state)\n                        current_architecture.append(action.item())\n                        \n                        # Store experience for PPO\n                        memories.append({\n                            'state': state.clone(),\n                            'action': action.unsqueeze(0),\n                            'log_prob': log_prob.unsqueeze(0),\n                            'reward': torch.zeros(1).to(device)  # Updated later\n                        })\n                \n                # Convert architecture to tensor for PPO mode\n                architecture_weights = torch.FloatTensor(current_architecture).to(device)\n                \n                # Train model with PPO-generated architecture\n                for inputs, targets in train_loader:\n                    batch_count += 1\n                    if batch_count % 10 == 0:\n                        print(f\"\\rPPO Training batch {batch_count}/{len(train_loader)}\", end=\"\")\n                    \n                    inputs, targets = inputs.to(device), targets.to(device)\n                    if torch.isnan(inputs).any():\n                        inputs = torch.nan_to_num(inputs, nan=0.0)\n                    \n                    # Update weights\n                    w_optimizer.zero_grad()\n                    outputs = model(inputs, architecture_weights)  # Use PPO architecture\n                    loss = F.cross_entropy(outputs, targets)\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.weights(), max_norm=1.0)\n                    w_optimizer.step()\n                    \n                    train_loss += loss.item()\n                \n                print()  # Line break after training\n                \n                # Evaluate architecture from PPO\n                val_eer = evaluate_architecture(model, val_loader, device, architecture_weights)\n                \n                # Update PPO controller based on performance\n                reward = best_val_eer - val_eer if val_eer < best_val_eer else 0\n                for memory in memories:\n                    memory['reward'] = torch.FloatTensor([reward]).to(device)\n                \n                # Update best architecture if improved\n                if val_eer < best_val_eer:\n                    best_val_eer = val_eer\n                    best_architecture = architecture_weights.clone()\n                    best_mode = 'ppo'\n                    print(f\"\\nNew best architecture found via PPO! EER: {best_val_eer:.4f}\")\n                \n                # Update PPO controller\n                if epoch % config[\"ppo_updates\"] == 0 and memories:\n                    actor_loss, critic_loss, entropy_loss = train_ppo(\n                        controller, controller_optimizer, memories)\n                    \n                    # Log PPO metrics\n                    wandb.log({\n                        \"actor_loss\": actor_loss,\n                        \"critic_loss\": critic_loss,\n                        \"entropy_loss\": entropy_loss\n                    })\n            \n            # DARTS optimization phase\n            else:\n                model.train()\n                train_loss = 0.0\n                batch_count = 0\n                \n                # Phase 1: Train model weights using DARTS approach\n                for inputs, targets in train_loader:\n                    batch_count += 1\n                    if batch_count % 10 == 0:\n                        print(f\"\\rDARTS Weight Training batch {batch_count}/{len(train_loader)}\", end=\"\")\n                    \n                    inputs, targets = inputs.to(device), targets.to(device)\n                    if torch.isnan(inputs).any():\n                        inputs = torch.nan_to_num(inputs, nan=0.0)\n                    \n                    # Update weights with internal alphas\n                    w_optimizer.zero_grad()\n                    outputs = model(inputs)  # Use alphas without external weights\n                    loss = F.cross_entropy(outputs, targets)\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.weights(), max_norm=1.0)\n                    w_optimizer.step()\n                    \n                    train_loss += loss.item()\n                \n                print()  # Line break after training\n                \n                # Phase 2: Update architecture parameters on validation set\n                model.train()  # Keep in train mode for alpha updates\n                val_batch_count = 0\n                \n                for inputs, targets in val_loader:\n                    # Use a subset of validation data\n                    if random.random() > 0.2:  # Sample ~20% for alpha updates\n                        continue\n                        \n                    val_batch_count += 1\n                    if val_batch_count > 50:  # Limit validation batches for speed\n                        break\n                    \n                    inputs, targets = inputs.to(device), targets.to(device)\n                    if torch.isnan(inputs).any():\n                        inputs = torch.nan_to_num(inputs, nan=0.0)\n                    \n                    # Update alphas\n                    alpha_optimizer.zero_grad()\n                    outputs = model(inputs)  # Use internal alphas\n                    loss = F.cross_entropy(outputs, targets)\n                    loss.backward()\n                    alpha_optimizer.step()\n                \n                # Evaluate DARTS architecture\n                val_eer = evaluate_architecture(model, val_loader, device, discrete=True)\n                \n                # Update best architecture if improved\n                if val_eer < best_val_eer:\n                    best_val_eer = val_eer\n                    # Save alphas as best architecture\n                    best_architecture = model._alphas.detach().clone()\n                    best_mode = 'darts'\n                    print(f\"\\nNew best architecture found via DARTS! EER: {best_val_eer:.4f}\")\n            \n            # Update learning rate for weights\n            w_scheduler.step()\n            \n            # Update epoch progress bar\n            epoch_pbar.update(1)\n            epoch_pbar.set_postfix({\n                \"Mode\": \"PPO\" if use_ppo else \"DARTS\",\n                \"Val EER\": f\"{val_eer:.4f}\",\n                \"Best EER\": f\"{best_val_eer:.4f}\"\n            })\n            \n            # Log metrics to wandb\n            avg_train_loss = train_loss / max(batch_count, 1)\n            wandb.log({\n                \"epoch\": epoch + 1,\n                \"train_loss\": avg_train_loss,\n                \"val_eer\": val_eer,\n                \"best_val_eer\": best_val_eer,\n                \"mode\": \"PPO\" if use_ppo else \"DARTS\",\n                \"learning_rate\": w_optimizer.param_groups[0]['lr']\n            })\n            \n            # Save checkpoint for the best model\n            if val_eer <= best_val_eer:\n                checkpoint_path = 'best_hybrid_model.pth'\n                if use_ppo:\n                    # Save PPO-generated architecture\n                    torch.save({\n                        'model_state_dict': model.state_dict(),\n                        'ppo_architecture': best_architecture,\n                        'eer': best_val_eer,\n                        'epoch': epoch + 1,\n                        'mode': 'PPO'\n                    }, checkpoint_path)\n                    # Add visualization during training\n                    if epoch % 5 == 0:  # Only create visualizations periodically to save time\n                        vis_path = visualize_ppo_architecture(\n                        best_architecture, \n                        num_cells=num_cells,\n                        num_nodes=num_nodes,\n                        num_ops=num_ops,\n                        save_path=f\"ppo_arch_epoch_{epoch}.png\",\n                        save_to_wandb=True\n                        )\n                else:\n                    # Save DARTS-generated architecture\n                    torch.save({\n                        'model_state_dict': model.state_dict(),\n                        'darts_alphas': model._alphas,\n                        'eer': best_val_eer,\n                        'epoch': epoch + 1,\n                        'mode': 'DARTS'\n                    }, checkpoint_path)\n                    # Add visualization during training\n                    if epoch % 5 == 0:  # Only create visualizations periodically\n                        vis_path = visualize_darts_architecture(\n                        model._alphas,\n                        num_cells=num_cells,\n                        num_nodes=num_nodes, \n                        num_ops=num_ops,\n                        save_path=f\"darts_arch_epoch_{epoch}.png\",\n                        save_to_wandb=True\n                        )\n                \n                # Log best model to wandb\n                wandb.save(checkpoint_path)\n    \n    # Finish wandb run\n    wandb.finish()\n    \n    # Return the best architecture (either from PPO or DARTS)\n    final_model = model\n    final_architecture = best_architecture\n    \n    return final_model, final_architecture, best_val_eer","metadata":{"execution":{"iopub.status.busy":"2025-04-25T00:09:57.091881Z","iopub.execute_input":"2025-04-25T00:09:57.092629Z","iopub.status.idle":"2025-04-25T00:09:57.116699Z","shell.execute_reply.started":"2025-04-25T00:09:57.092604Z","shell.execute_reply":"2025-04-25T00:09:57.116146Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def evaluate_model(model, architecture, test_loader, device, log_to_wandb=True):\n    \"\"\"Evaluate the model with the best architecture\"\"\"\n    model.eval()\n    all_targets = []\n    all_scores = []\n    test_loss = 0.0\n    correct = 0\n    total = 0\n    \n    # Determine if architecture is from PPO or DARTS\n    is_ppo_arch = architecture.dim() == 1\n    \n    # Create a progress bar for evaluation\n    eval_pbar = tqdm(test_loader, desc=\"Evaluating\")\n    \n    with torch.no_grad():\n        for inputs, targets in eval_pbar:\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Choose correct evaluation mode\n            if is_ppo_arch:\n                # PPO architecture\n                outputs = model(inputs, architecture)\n            else:\n                # DARTS architecture - set model's alphas and use discrete mode\n                model._alphas.data = architecture.data\n                outputs = model(inputs, discrete=True)\n            \n            # Compute loss\n            loss = F.cross_entropy(outputs, targets)\n            test_loss += loss.item()\n            \n            # Compute accuracy\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Get scores for EER calculation\n            scores = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n            all_targets.extend(targets.cpu().numpy())\n            all_scores.extend(scores)\n            \n            # Update progress bar\n            eval_pbar.set_postfix({\n                \"loss\": f\"{loss.item():.4f}\",\n                \"accuracy\": f\"{100.0 * correct / total:.2f}%\"\n            })\n    \n    # Calculate test accuracy\n    test_accuracy = 100.0 * correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    \n    # Calculate EER\n    try:\n        fpr, tpr, thresholds = roc_curve(all_targets, all_scores, pos_label=1)\n        fnr = 1 - tpr\n        idx = np.nanargmin(np.absolute(fnr - fpr))\n        eer = (fpr[idx] + fnr[idx]) / 2\n        eer_threshold = thresholds[idx]\n    except Exception as e:\n        print(f\"Error calculating EER: {e}\")\n        eer = 0.5\n        eer_threshold = 0.5\n    \n    # Plot ROC curve\n    plt.figure(figsize=(10, 8))\n    plt.plot(fpr, tpr, label=f'ROC Curve (EER = {eer:.4f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve for Deepfake Detection')\n    plt.legend(loc=\"lower right\")\n    \n    # Save ROC curve\n    roc_curve_path = 'roc_curve.png'\n    plt.savefig(roc_curve_path)\n    \n    # Print results\n    print(f\"\\nTest Results - Loss: {avg_test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, EER: {eer:.4f}\")\n    \n    # Log to wandb if requested\n    if log_to_wandb and wandb.run is not None:\n        wandb.log({\n            \"test_loss\": avg_test_loss,\n            \"test_accuracy\": test_accuracy,\n            \"test_eer\": eer,\n            \"eer_threshold\": eer_threshold,\n            \"roc_curve\": wandb.Image(roc_curve_path)\n        })\n        \n        # Log confusion matrix\n        cm = np.zeros((2, 2))\n        for i in range(len(all_targets)):\n            pred_class = 1 if all_scores[i] > eer_threshold else 0\n            cm[all_targets[i]][pred_class] += 1\n        \n        # Normalize confusion matrix\n        cm_norm = cm / np.maximum(cm.sum(axis=1, keepdims=True), 1e-8)\n        \n        # Plot confusion matrix\n        plt.figure(figsize=(8, 6))\n        plt.imshow(cm_norm, cmap='Blues')\n        plt.colorbar()\n        plt.title('Normalized Confusion Matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.xticks([0, 1], ['Bonafide', 'Spoof'])\n        plt.yticks([0, 1], ['Bonafide', 'Spoof'])\n        \n        # Add text annotations\n        for i in range(2):\n            for j in range(2):\n                plt.text(j, i, f'{cm[i, j]:.0f}\\n({cm_norm[i, j]:.2f})', \n                         ha='center', va='center', \n                         color='white' if cm_norm[i, j] > 0.5 else 'black')\n        \n        cm_path = 'confusion_matrix.png'\n        plt.savefig(cm_path)\n        wandb.log({\"confusion_matrix\": wandb.Image(cm_path)})\n    \n    return eer, eer_threshold","metadata":{"execution":{"iopub.status.busy":"2025-04-25T00:10:02.515702Z","iopub.execute_input":"2025-04-25T00:10:02.516301Z","iopub.status.idle":"2025-04-25T00:10:02.528579Z","shell.execute_reply.started":"2025-04-25T00:10:02.516276Z","shell.execute_reply":"2025-04-25T00:10:02.527753Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ===========================\n# Architecture Visualization\n# ===========================\nprint(\"Visualizing discovered architectures...\")\n\n# Add the visualization code here\n# [Insert the entire visualization code I provided]\n\n# Then add the usage code:\nif wandb.run is not None:\n    print(\"Generating visualizations for best architecture...\")\n    \n    # Load best architecture from checkpoint\n    checkpoint_path = 'best_hybrid_model.pth'\n    \n    if os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path)\n        \n        # Determine architecture type\n        if 'ppo_architecture' in checkpoint:\n            architecture = checkpoint['ppo_architecture']\n            is_ppo = True\n            best_mode = 'PPO'\n        elif 'darts_alphas' in checkpoint:\n            architecture = checkpoint['darts_alphas']\n            is_ppo = False\n            best_mode = 'DARTS'\n        \n        # Create visualizations\n        if is_ppo:\n            save_path = visualize_ppo_architecture(architecture, save_to_wandb=True)\n        else:\n            save_path = visualize_darts_architecture(architecture, save_to_wandb=True)\n            \n        # Analyze architecture statistics\n        stats = analyze_architecture_statistics(architecture, is_ppo=is_ppo)\n        plot_architecture_statistics(stats, save_to_wandb=True)\n        \n        # Print key findings\n        patterns = stats[\"patterns\"]\n        print(f\"Architecture Analysis ({best_mode}):\")\n        print(f\"Most common operation: {patterns['most_common_op']} ({patterns['most_common_percentage']:.1f}%)\")\n        \n        if patterns[\"cell_specific_patterns\"]:\n            print(\"Cell-specific patterns detected!\")\n    else:\n        print(f\"Checkpoint file {checkpoint_path} not found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T00:10:05.526429Z","iopub.execute_input":"2025-04-25T00:10:05.526901Z","iopub.status.idle":"2025-04-25T00:10:05.533221Z","shell.execute_reply.started":"2025-04-25T00:10:05.526876Z","shell.execute_reply":"2025-04-25T00:10:05.532545Z"}},"outputs":[{"name":"stdout","text":"Visualizing discovered architectures...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import roc_curve\nimport soundfile as sf\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport librosa\nimport random\nfrom torch.distributions import Categorical\nimport wandb\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef setup_progress_monitoring():\n    \"\"\"Setup enhanced progress monitoring\"\"\"\n    import threading\n    import time\n    import psutil\n    import os\n    \n    def monitor_resources():\n        process = psutil.Process(os.getpid())\n        start_time = time.time()\n        while True:\n            try:\n                # CPU usage\n                cpu_percent = process.cpu_percent(interval=1)\n                # Memory usage\n                memory_info = process.memory_info()\n                memory_mb = memory_info.rss / (1024 * 1024)\n                # GPU memory if available\n                gpu_memory_mb = 0\n                try:\n                    if torch.cuda.is_available():\n                        gpu_memory_mb = torch.cuda.memory_allocated() / (1024 * 1024)\n                except:\n                    pass\n                \n                elapsed = time.time() - start_time\n                hours, remainder = divmod(elapsed, 3600)\n                minutes, seconds = divmod(remainder, 60)\n                \n                print(f\"\\r[{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}] \"\n                      f\"CPU: {cpu_percent:.1f}% | RAM: {memory_mb:.1f} MB | \"\n                      f\"GPU: {gpu_memory_mb:.1f} MB\", end=\"\", flush=True)\n                \n                time.sleep(5)  # Update every 5 seconds\n            except:\n                break\n    \n    # Start monitoring in a background thread\n    monitor_thread = threading.Thread(target=monitor_resources, daemon=True)\n    monitor_thread.start()\n    print(\"Resource monitoring started...\")\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n# Add functions to handle NumPy types for JSON serialization\ndef convert_numpy_types(obj):\n    \"\"\"Convert NumPy types to Python native types for JSON serialization\"\"\"\n    if isinstance(obj, (np.int32, np.int64)):\n        return int(obj)\n    elif isinstance(obj, (np.float32, np.float64)):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    return obj\n\ndef numpy_safe_json_dump(obj, f, indent=4):\n    \"\"\"JSON dump that handles NumPy types\"\"\"\n    class NumpyEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, np.integer):\n                return int(obj)\n            if isinstance(obj, np.floating):\n                return float(obj)\n            if isinstance(obj, np.ndarray):\n                return obj.tolist()\n            return super().default(obj)\n    \n    json.dump(obj, f, indent=indent, cls=NumpyEncoder)\n\n# Recursive function to convert all NumPy values in nested dictionaries/lists\ndef convert_dict_numpy_to_python(obj):\n    if isinstance(obj, dict):\n        return {k: convert_dict_numpy_to_python(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_dict_numpy_to_python(item) for item in obj]\n    else:\n        return convert_numpy_types(obj)\n\n# Main function follows\ndef main():\n    # Start the resource monitoring first\n    setup_progress_monitoring()\n    \n    # Then set the random seed\n    set_seed()\n    \n    # Import time for experiment naming\n    import time\n    import os\n    \n    # Initialize wandb for the entire experiment\n    experiment_name = f\"ASVspoof2019_NAS_{int(time.time())}\"\n    \n    print(\"Starting Deepfake Audio Detection with NAS\")\n    print(\"=\" * 80)\n    \n    # Paths and parameters (based on the provided dataset structure)\n    base_dir = \"/kaggle/input/asvspoof-dataset-2019\"\n    data_dir_train = os.path.join(base_dir, \"LA\", \"ASVspoof2019_LA_train\", \"flac\")\n    data_dir_dev = os.path.join(base_dir, \"LA\", \"ASVspoof2019_LA_dev\", \"flac\")\n    data_dir_eval = os.path.join(base_dir, \"LA\", \"ASVspoof2019_LA_eval\", \"flac\")\n    \n    train_protocol = os.path.join(base_dir, \"LA\", \"ASVspoof2019_LA_cm_protocols\", \"ASVspoof2019.LA.cm.train.trn.txt\")\n    dev_protocol = os.path.join(base_dir, \"LA\", \"ASVspoof2019_LA_cm_protocols\", \"ASVspoof2019.LA.cm.dev.trl.txt\")\n    eval_protocol = os.path.join(base_dir, \"LA\", \"ASVspoof2019_LA_cm_protocols\", \"ASVspoof2019.LA.cm.eval.trl.txt\")\n    \n    # Log dataset information\n    print(f\"Train data directory: {data_dir_train}\")\n    print(f\"Train protocol file: {train_protocol}\")\n    print(f\"Dev data directory: {data_dir_dev}\")\n    print(f\"Dev protocol file: {dev_protocol}\")\n    print(f\"Eval data directory: {data_dir_eval}\")\n    print(f\"Eval protocol file: {eval_protocol}\")\n    \n    # Experiment configuration\n    feature_type = 'mfcc'  # Options: 'mfcc', 'spec', 'cqt'\n    max_seq_len = 400\n    batch_size_train = 32\n    batch_size_eval = 64\n    num_workers = 4\n    \n    # Select search method: 'hybrid' for PPO+DARTS\n    search_method = 'hybrid'\n\n    # Initialize wandb with your API key (replace with your actual key)\n    wandb.login(key=\"373119fa114178ac5e09f36834a61c071debfbb8\")\n    \n    wandb.init(\n        project=\"ASVspoof2019-NAS\",\n        name=f\"{experiment_name}_{search_method}\",\n        config={\n            \"feature_type\": feature_type,\n            \"max_sequence_length\": max_seq_len,\n            \"batch_size_train\": batch_size_train,\n            \"batch_size_eval\": batch_size_eval,\n            \"num_workers\": num_workers,\n            \"dataset\": \"ASVspoof2019 LA\",\n            \"search_method\": search_method\n        }\n    )\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    \n    print(\"Loading training dataset...\")\n    train_dataset = ASVSpoofDataset(\n        root_dir=data_dir_train,\n        protocol_file=train_protocol,\n        feature_type=feature_type,\n        max_len=max_seq_len,\n        is_train=True,\n        use_subsampling=False  # Now using full dataset\n    )\n    \n    print(\"Loading validation dataset...\")\n    dev_dataset = ASVSpoofDataset(\n        root_dir=data_dir_dev,\n        protocol_file=dev_protocol,\n        feature_type=feature_type,\n        max_len=max_seq_len,\n        is_train=False\n    )\n    \n    print(\"Loading evaluation dataset...\")\n    eval_dataset = ASVSpoofDataset(\n        root_dir=data_dir_eval,\n        protocol_file=eval_protocol,\n        feature_type=feature_type,\n        max_len=max_seq_len,\n        is_train=False\n    )\n    \n    # Log dataset sizes\n    print(f\"Training dataset size: {len(train_dataset)} samples\")\n    print(f\"Validation dataset size: {len(dev_dataset)} samples\")\n    print(f\"Evaluation dataset size: {len(eval_dataset)} samples\")\n    wandb.log({\n        \"train_dataset_size\": len(train_dataset),\n        \"val_dataset_size\": len(dev_dataset),\n        \"eval_dataset_size\": len(eval_dataset)\n    })\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size_train, \n        shuffle=True, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    dev_loader = DataLoader(\n        dev_dataset, \n        batch_size=batch_size_eval, \n        shuffle=False, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    eval_loader = DataLoader(\n        eval_dataset, \n        batch_size=batch_size_eval, \n        shuffle=False, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    # Architecture search parameters\n    nas_config = {\n        \"input_channels\": 60,  # 20 MFCCs x 3 (static, delta, delta-delta)\n        \"num_cells\": 3,\n        \"num_nodes\": 4,\n        \"num_ops\": 10,  # Expanded to 10 operations\n        \"epochs\": 30,\n        \"ppo_updates\": 5,\n        \"project_name\": \"ASVspoof2019-NAS\"\n    }\n    \n    # Log NAS configuration\n    print(\"\\nNeural Architecture Search Configuration:\")\n    for key, value in nas_config.items():\n        print(f\"  {key}: {value}\")\n    \n    # Create output directory for results\n    output_dir = f\"/kaggle/working/results_{experiment_name}_{search_method}\"\n    os.makedirs(output_dir, exist_ok=True)\n    print(f\"\\nResults will be saved to {output_dir}\")\n    \n    # Perform architecture search using hybrid PPO-DARTS approach\n    print(f\"\\nStarting Neural Architecture Search using {search_method.upper()} method...\")\n    \n    model, best_architecture, best_val_eer = search_architecture_hybrid(\n        train_loader=train_loader,\n        val_loader=dev_loader,\n        device=device,\n        input_channels=nas_config[\"input_channels\"],\n        num_cells=nas_config[\"num_cells\"],\n        num_nodes=nas_config[\"num_nodes\"],\n        num_ops=nas_config[\"num_ops\"],\n        epochs=nas_config[\"epochs\"],\n        ppo_updates=nas_config[\"ppo_updates\"],\n        project_name=nas_config[\"project_name\"] + \"-hybrid\"\n    )\n    \n    # Save best architecture\n    torch.save(best_architecture, os.path.join(output_dir, \"best_architecture.pt\"))\n    \n    # Initialize a new wandb run for final evaluation\n    wandb.finish()  # Finish the NAS run\n    wandb.init(\n        project=\"ASVspoof2019-NAS\",\n        name=f\"{experiment_name}_{search_method}_final_evaluation\",\n        config={\n            \"feature_type\": feature_type,\n            \"best_val_eer\": float(best_val_eer),  # Convert to Python float\n            \"search_method\": search_method\n        }\n    )\n    \n    # Evaluate on the evaluation set\n    print(\"\\nPerforming final evaluation on test set...\")\n    test_eer, eer_threshold = evaluate_model(model, best_architecture, eval_loader, device)\n    \n    # Log final metrics - convert NumPy types to Python native types\n    wandb.log({\n        \"final_test_eer\": float(test_eer),\n        \"eer_threshold\": float(eer_threshold)\n    })\n    \n    # Visualize the architecture with annotations\n    print(\"\\nVisualizing the best architecture...\")\n    # Check if it's a PPO or DARTS architecture\n    is_ppo_arch = best_architecture.dim() == 1\n    \n    if is_ppo_arch:\n        fig_path = visualize_ppo_architecture(\n            best_architecture, \n            num_cells=nas_config[\"num_cells\"], \n            num_nodes=nas_config[\"num_nodes\"], \n            num_ops=nas_config[\"num_ops\"],\n            save_to_wandb=True\n        )\n    else:\n        fig_path = visualize_darts_architecture(\n            best_architecture, \n            num_cells=nas_config[\"num_cells\"], \n            num_nodes=nas_config[\"num_nodes\"], \n            num_ops=nas_config[\"num_ops\"],\n            save_to_wandb=True\n        )\n    \n    # Save architecture visualization\n    import shutil\n    shutil.copy(fig_path, os.path.join(output_dir, \"architecture_visualization.png\"))\n    \n    # Create a summary report - ensure all values are Python native types\n    summary = {\n        \"experiment_name\": experiment_name,\n        \"search_method\": search_method,\n        \"feature_type\": feature_type,\n        \"best_validation_eer\": float(best_val_eer),\n        \"test_eer\": float(test_eer),\n        \"eer_threshold\": float(eer_threshold),\n        \"model_architecture\": {\n            \"num_cells\": nas_config[\"num_cells\"],\n            \"num_nodes\": nas_config[\"num_nodes\"],\n            \"num_operations\": nas_config[\"num_ops\"]\n        }\n    }\n        \n    # Save summary as JSON using the NumPy-safe encoder\n    with open(os.path.join(output_dir, \"summary.json\"), \"w\") as f:\n        numpy_safe_json_dump(summary, f)\n    \n    # Also save as text for easy reading\n    with open(os.path.join(output_dir, \"summary.txt\"), \"w\") as f:\n        f.write(\"ASVspoof 2019 Deepfake Detection Summary\\n\")\n        f.write(\"=\" * 50 + \"\\n\\n\")\n        f.write(f\"Experiment name: {experiment_name}\\n\")\n        f.write(f\"Search method: {search_method.upper()}\\n\")\n        f.write(f\"Feature type: {feature_type}\\n\\n\")\n        f.write(\"Performance metrics:\\n\")\n        f.write(f\"  Best validation EER: {float(best_val_eer):.4f}\\n\")\n        f.write(f\"  Test EER: {float(test_eer):.4f}\\n\")\n        f.write(f\"  EER threshold: {float(eer_threshold):.4f}\\n\\n\")\n        f.write(\"Model architecture:\\n\")\n        f.write(f\"  Number of cells: {nas_config['num_cells']}\\n\")\n        f.write(f\"  Number of nodes per cell: {nas_config['num_nodes']}\\n\")\n        f.write(f\"  Number of operations: {nas_config['num_ops']}\\n\")\n    \n    # Log summary to wandb\n    wandb.save(os.path.join(output_dir, \"summary.txt\"))\n    wandb.save(os.path.join(output_dir, \"summary.json\"))\n    \n    print(\"\\nExperiment completed!\")\n    print(f\"Final Test EER: {float(test_eer):.4f}, EER Threshold: {float(eer_threshold):.4f}\")\n    print(f\"All results saved to {output_dir}\")\n    print(\"=\" * 80)\n    \n    # Finish wandb\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2025-04-25T00:15:03.922842Z","iopub.execute_input":"2025-04-25T00:15:03.923440Z","execution_failed":"2025-04-25T03:34:58.666Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtangridhruv\u001b[0m (\u001b[33mtangridhruv-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"name":"stdout","text":"Resource monitoring started...\nStarting Deepfake Audio Detection with NAS\n================================================================================\nTrain data directory: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_train/flac\nTrain protocol file: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\nDev data directory: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_dev/flac\nDev protocol file: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\nEval data directory: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_eval/flac\nEval protocol file: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\n[00:04:55] CPU: 1.0% | RAM: 648.7 MB | GPU: 0.0 MB","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250425_001504-06jtkb84</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tangridhruv-carnegie-mellon-university/ASVspoof2019-NAS/runs/06jtkb84' target=\"_blank\">ASVspoof2019_NAS_1745540103_hybrid</a></strong> to <a href='https://wandb.ai/tangridhruv-carnegie-mellon-university/ASVspoof2019-NAS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tangridhruv-carnegie-mellon-university/ASVspoof2019-NAS' target=\"_blank\">https://wandb.ai/tangridhruv-carnegie-mellon-university/ASVspoof2019-NAS</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tangridhruv-carnegie-mellon-university/ASVspoof2019-NAS/runs/06jtkb84' target=\"_blank\">https://wandb.ai/tangridhruv-carnegie-mellon-university/ASVspoof2019-NAS/runs/06jtkb84</a>"},"metadata":{}},{"name":"stdout","text":"[00:00:07] CPU: 46.0% | RAM: 649.9 MB | GPU: 0.0 MBCreating datasets...\nLoading training dataset...\nReading protocol file: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\n","output_type":"stream"},{"name":"stderr","text":"Loading training protocol: 100%|| 25380/25380 [00:00<00:00, 1441552.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset loaded: 25380 samples (2580 bonafide, 22800 spoof)\nSubsampling training data for faster NAS...\nSubsampled to 5000 samples\nLoading validation dataset...\nReading protocol file: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\n","output_type":"stream"},{"name":"stderr","text":"Loading evaluation protocol: 100%|| 24844/24844 [00:00<00:00, 1674970.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset loaded: 24844 samples (2548 bonafide, 22296 spoof)\nLoading evaluation dataset...\nReading protocol file: /kaggle/input/asvspoof-dataset-2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\n","output_type":"stream"},{"name":"stderr","text":"Loading evaluation protocol: 100%|| 71237/71237 [00:00<00:00, 1560870.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset loaded: 71237 samples (7355 bonafide, 63882 spoof)\nTraining dataset size: 5000 samples\nValidation dataset size: 24844 samples\nEvaluation dataset size: 71237 samples\n\nNeural Architecture Search Configuration:\n  input_channels: 60\n  num_cells: 3\n  num_nodes: 4\n  num_ops: 10\n  epochs: 30\n  ppo_updates: 5\n  project_name: ASVspoof2019-NAS\n\nResults will be saved to /kaggle/working/results_ASVspoof2019_NAS_1745540103_hybrid\n\nStarting Neural Architecture Search using HYBRID method...\nInitializing PPO controller with state_dim=1, action_dim=10\n","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:   0%|          | 0/30 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[00:03:07] CPU: 99.0% | RAM: 1449.3 MB | GPU: 430.3 MBBB\n[00:12:49] CPU: 100.0% | RAM: 1552.8 MB | GPU: 102.3 MBB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:   3%|         | 1/30 [12:39<6:07:09, 759.63s/it, Mode=DARTS, Val EER=0.2245, Best EER=0.2245]","output_type":"stream"},{"name":"stdout","text":"\nNew best architecture found via DARTS! EER: 0.2245\n[00:20:07] CPU: 100.0% | RAM: 1751.4 MB | GPU: 609.5 MBB\n[00:21:55] CPU: 100.0% | RAM: 1753.2 MB | GPU: 128.7 MB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:   7%|         | 2/30 [21:48<4:56:33, 635.50s/it, Mode=PPO, Val EER=0.0730, Best EER=0.0730]  ","output_type":"stream"},{"name":"stdout","text":"\nNew best architecture found via PPO! EER: 0.0730\n[00:29:13] CPU: 101.0% | RAM: 1755.4 MB | GPU: 501.8 MBB\n[00:31:02] CPU: 100.0% | RAM: 1755.4 MB | GPU: 115.9 MB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:  10%|         | 3/30 [30:52<4:27:19, 594.06s/it, Mode=PPO, Val EER=0.0804, Best EER=0.0730]","output_type":"stream"},{"name":"stdout","text":"[00:33:32] CPU: 100.0% | RAM: 1755.4 MB | GPU: 789.2 MBB\n[00:47:20] CPU: 100.0% | RAM: 1755.4 MB | GPU: 127.9 MBB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:  13%|        | 4/30 [42:21<4:33:30, 631.16s/it, Mode=DARTS, Val EER=0.2396, Best EER=0.0730]","output_type":"stream"},{"name":"stdout","text":"[00:49:50] CPU: 100.0% | RAM: 1755.4 MB | GPU: 80.9 MBMB\n[00:54:03] CPU: 100.0% | RAM: 1755.5 MB | GPU: 102.9 MBB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:  17%|        | 5/30 [53:55<4:32:31, 654.08s/it, Mode=DARTS, Val EER=0.2746, Best EER=0.0730]","output_type":"stream"},{"name":"stdout","text":"[00:56:33] CPU: 100.0% | RAM: 1755.5 MB | GPU: 1883.9 MB\n[01:05:21] CPU: 100.0% | RAM: 1755.5 MB | GPU: 140.4 MBB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:  20%|        | 6/30 [1:05:12<4:24:43, 661.82s/it, Mode=DARTS, Val EER=0.5000, Best EER=0.0730]","output_type":"stream"},{"name":"stdout","text":"[01:07:51] CPU: 101.0% | RAM: 1755.5 MB | GPU: 1172.8 MB\n[01:16:45] CPU: 100.0% | RAM: 1755.5 MB | GPU: 116.7 MBB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:  23%|       | 7/30 [1:16:37<4:16:37, 669.47s/it, Mode=DARTS, Val EER=0.5000, Best EER=0.0730]","output_type":"stream"},{"name":"stdout","text":"[01:24:03] CPU: 100.0% | RAM: 1755.5 MB | GPU: 1610.5 MB\n[01:25:52] CPU: 100.0% | RAM: 1755.5 MB | GPU: 103.7 MB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:  27%|       | 8/30 [1:25:44<3:51:11, 630.50s/it, Mode=PPO, Val EER=0.1698, Best EER=0.0730]  ","output_type":"stream"},{"name":"stdout","text":"[01:28:22] CPU: 99.9% | RAM: 1755.5 MB | GPU: 400.7 MBMB\n[01:37:16] CPU: 100.0% | RAM: 1755.5 MB | GPU: 127.9 MBB","output_type":"stream"},{"name":"stderr","text":"Hybrid Search Progress:  30%|       | 9/30 [1:37:07<3:46:22, 646.77s/it, Mode=DARTS, Val EER=0.2582, Best EER=0.0730]","output_type":"stream"},{"name":"stdout","text":"[01:38:40] CPU: 101.0% | RAM: 1755.6 MB | GPU: 2421.4 MB","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}